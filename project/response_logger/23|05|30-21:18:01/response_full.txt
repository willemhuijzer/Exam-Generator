{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "Question 1:\nWhy is text normalization important in natural language processing?\nA. To increase the variations of a word in the raw text\nB. To decrease the efficiency of the model\nC. To reduce the dimensionality of the input\nD. To increase false negatives in the model\n\nAnswer: C\n\nQuestion 2:\nWhich of the following is NOT a component of text normalization?\nA. Sentence structure\nB. Punctuation\nC. Word sense ambiguity\nD. Tokenization\n\nAnswer: C\n\nQuestion 3:\nWhat is the purpose of stemming in text normalization?\nA. To reduce words to base word (lemma)\nB. To reduce words to word stem or root form\nC. To segment running text into words and sequences\nD. To remove all punctuation\n\nAnswer: B\n\nQuestion 4:\nWhich of the following is an intrinsic evaluation measure for language models?\nA. Measuring performance on a downstream application\nB. Entropy\nC. Cross-entropy\nD. Learning to encode similarity in the vectors\n\nAnswer: B\n\nQuestion 5:\nWhat is the difference between WordNet and distributional semantics?\nA. WordNet encodes word meanings based on the context they appear in, while distributional semantics uses a lexicon of synonyms\nB. WordNet is subjective, while distributional semantics is objective\nC. WordNet is used for word analytics, while distributional semantics focuses on sentence structure\nD. WordNet is based on a database of word relationships, while distributional semantics uses co-occurrence based context vectors\n\nAnswer: D\n\nQuestion 6:\nWhat is the objective function in Word2Vec?\nA. To maximize the likelihood of the center word given the context words\nB. To minimize the distance between co-occurrence based context vectors\nC. To maximize the probability of the context words given the center word\nD. To minimize the entropy of the vocabulary\n\nAnswer: C\n\nQuestion 7:\nWhat is the main advantage of dense word embeddings?\nA. They have many parameters, which improves generalization\nB. They have a large vocabulary, which improves semantic similarity\nC. They can be easily included as features in machine learning systems\nD. They are better at capturing context than co-occurrence based context vectors\n\nAnswer: C\n\nQuestion 8:\nWhat is the difference between stemming and lemmatization?\nA. Stemming reduces words to their root form, while lemmatization reduces words to base words\nB. Stemming and lemmatization are the same thing\nC. Stemming reduces words to base words, while lemmatization reduces words to their root form\nD. Stemming and lemmatization reduce words to synonyms\n\nAnswer: A\n\nQuestion 9:\nWhich of the following is an example of a hypernymy relation?\nA. Car is a kind of vehicle\nB. An air bag is part of a car\nC. A tomato is a type of fruit\nD. A book is related to an author\n\nAnswer: A\n\nQuestion 10:\nWhy can MLE not distinguish between grammatical and ungrammatical sentences in estimating the probability of a sentence in English?\nA. It is based on a large corpus of general English text\nB. It assumes that anything that hasn't occurred will never occur\nC. It assigns zero probability to any sentence that has never occurred in a corpus\nD. It uses the count of a sentence in a large dataset to estimate its probability\n\nAnswer: D",
        "role": "assistant"
      }
    }
  ],
  "created": 1685474282,
  "id": "chatcmpl-7Lz90gh3T0JoSgHUFFp9IjDdRfPVO",
  "model": "gpt-3.5-turbo-0301",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 717,
    "prompt_tokens": 2389,
    "total_tokens": 3106
  }
}