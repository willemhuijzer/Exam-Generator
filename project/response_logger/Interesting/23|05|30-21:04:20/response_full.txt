{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "Question 1:\nWhy do we need text normalization?\nA. To increase the variations of a word in raw text\nB. To decrease the variations of a word in raw text and increase efficiency\nC. To eliminate all punctuation from the text\nD. To increase the dimensionality of input in structures like BOW and word embeddings\n\nAnswer: B\n\nQuestion 2:\nWhich of the following is NOT a part of text normalization?\nA. Tokenization\nB. Sentence structure normalization\nC. Punctuation normalization\nD. Tone analysis\n\nAnswer: D\n\nQuestion 3:\nWhat is the purpose of stemming in NLP?\nA. To increase the variations of words in raw text\nB. To reduce words to their root form or stem\nC. To remove all punctuation from the text\nD. To generate context-dependent paraphrases\n\nAnswer: B\n\nQuestion 4:\nWhich of the following models attempts to learn the underlying word representation relative to its context/surrounding words?\nA. Bag of Words\nB. Continuous BOW\nC. Word2Vec\nD. Naive Bayes\n\nAnswer: B\n\nQuestion 5:\nWhat is the maximum likelihood estimation (MLE) method used for in NLP?\nA. To determine values for the parameters of a model\nB. To classify text data\nC. To reduce the dimensionality of the input for structures like BOW and word embeddings\nD. To eliminate all variance in the raw text\n\nAnswer: A\n\nQuestion 6:\nWhat is an intrinsic evaluation measure for language models?\nA. Cross-entropy\nB. Perplexity\nC. Entropy\nD. Cosine similarity\n\nAnswer: B\n\nQuestion 7:\nWhat is WordNet?\nA. A software for vector arithmetic\nB. A long list of words in a fixed vocabulary\nC. A database of A is-a B relationships\nD. A neural network embedding method\n\nAnswer: C\n\nQuestion 8:\nWhat is the primary advantage of dense word embeddings?\nA. They are easier to include as features in machine learning systems\nB. They have fewer parameters, better generalization and less overfitting\nC. They are better at capturing synonymy\nD. All of the above\n\nAnswer: D\n\nQuestion 9:\nWhat is the most popular family of neural network embeddings?\nA. GloVe\nB. FastText\nC. Word2Vec\nD. ELMo\n\nAnswer: C\n\nQuestion 10:\nWhat is the objective function in Word2Vec?\nA. To maximize the predictive accuracy of center words given their context\nB. To minimize the dimensionality of input for BOW and word embeddings\nC. To eliminate ambiguity in word sense\nD. To generate context-dependent paraphrases\n\nAnswer: A",
        "role": "assistant"
      }
    }
  ],
  "created": 1685473461,
  "id": "chatcmpl-7Lyvls0NOMpdXo0LAF8Euxs30naTB",
  "model": "gpt-3.5-turbo-0301",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 567,
    "prompt_tokens": 2389,
    "total_tokens": 2956
  }
}