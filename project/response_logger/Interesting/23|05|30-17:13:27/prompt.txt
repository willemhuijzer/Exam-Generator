CONTENT OF LECTURE SLIDES:

What property/properties of language does the example below (not) illustrate and why?
“There’s a mosquito buzzing in my ear. Nothing is more irritating than a buzzing sound.”

Which aspect of this utterance makes it hard for NLP tasks?
“I’m soooo glad my computer arrived one month after classes started… tbh I’m fine.”

Datasets form the basis for training, evaluating, and benchmarking machine learning models. The ways in which we collect, construct, and share these datasets inform the kinds of problems the field pursues, and the methods explored in algorithm development. 

The goal of sentiment analysis is to predict the sentiment expressed in a piece of text. Why is this so hard? Sentiment is a measure of a speaker’s private state, which is unobservable. Sometimes words are a good indicator of sentiment but many times it requires deep world + contextual knowledge. 

Text classification: a mapping h from input data x (drawn from instance space X) to a label (or labels) y from some enumerable output space Y. 
Before you build a system, choose a dataset for evaluation. Why is data-driven evaluation important?
	Good science requires controlled experimentation.
	Good engineering requires benchmarks.
	Your intuitions about typical inputs are probably wrong.

Many corpora are prepared specifically for linguistics/NLP research with text from content providers (e.g., newspapers). In fact, there is an entire subfield devoted to developing new language resources. You can also collect a new one by, e.g., scraping websites.

To evaluate and compare sentiment analyzers, we need reviews with gold labels (+ or -) attached. These can be: 
	Derived automatically from the original data artifact (metadata such as star ratings) or
	Added by a human annotator who reads the text 
Supervised learning: given training data in the form of <x,y> pairs, learn h ̂(x).

Once we have a dataset with gold (correct) labels, we can give the text of each review as input to our system and measure how often its output matches the gold label. Simplest measure:
accuracy=(#correct)/(#total)
However, accuracy is not a good measure when we have any kind of class imbalance. We’d also like to see the quality of the predictions our system makes (e.g., # and type of prediction errors made). 

Is this tweet ironic?
	precision=TP/(TP+FP). How often is my prediction correct?
	A not precise model may find a lot of the positives, but its selection method is noisy: it wrongly detects many positives that aren’t actually positives.
	A precise model is very “pure”: maybe it does not find all the positives, but the ones that the model does class as positive are very likely to be correct.
	recall=TP/(TP+FN). How many of the ironic tweets do I find? 
	A model with high recall succeeds well in finding all the positive cases in the data, even though they may also wrongly identify some negative cases as positive cases.
	A model with low recall is not able to find all (or a large part) of the positive cases in the data. 

Tuning for high precision means the system should not make a mistake.
Tuning for high recall means the system should not miss a case.

In most NLP systems, we are looking for a good trade-off between precision and recall. 
Harmonic mean: 
F1=2*(precision*recall)/(precision+recall)
F1 is commonly used for (supervised) classification tasks.

Random baseline: randomly assign a label to each instance.
	Fix a random seed, repeat n times, average the results.
Majority baseline: determine majority class.
	Assign the majority label to all instances, calculate the results.

Which system is better? Calculate the F1-measure for these two systems. 






Class labels: 1 (ironic by clash), 2 (situational irony), 3 (other irony), 0 (not ironic).






	Calculate precision and recall for every class separately
	Clash versus Situational/Other/Not ironic






	Situational versus Clash/Other/Not ironic






	Etc.

	Average the results over all classes:
	Macro-average: does not take class imbalance into account.
	Weighted: average over classes, weighted by class size (support).

	Make sure to check which information is displayed in rows and columns (prediction versus gold)! This varies depending on the implementation and is crucial for determining the positives and negatives.

Now we have a definition of the sentiment analysis task (inputs and outputs) and a basic way to evaluate a sentiment analyzer (accuracy on gold data). But, we are still missing an algorithm for predicting sentiment.

A simple sentiment classification algorithm is to use a sentiment lexicon to count positive and negative words and predict whichever is greater. 
Problems with simply counting:
	Hard to know whether words that seem positive or negative tend to actually be used that way:
	Sense ambiguity
	Sarcasm/irony
	Text could mention expectations or opposing viewpoints, in contrast to author’s actual opinion.
	Opinion words may be describing (e.g.) a character’s attitude rather than an evaluation of the film.
	Some words act as semantic modifiers of other opinion-bearing words/phrases, so interpreting the full meaning requires sophistication:
	“I can’t stand this movie” versus “I can’t believe how great this movie is”

Perhaps corpora can help address the first objection by using frequency counts to ascertain which words tend to be positive or negative. 

Text classification: a mapping h from input data x (drawn from instance space X) to a label (or labels) y from some enumerable output space Y.  The classification function that we want to learn has two different components:
	The representation of the data
	The formal structure of the learning method (what’s the relationship between the input and output)  Naive Bayes, logistic regression, convolutional neural network, etc.

Representation for sentiment analysis:
	Only positive/negative words in MPQA
	Only words in isolation (bag of words)
	Simplest representation. Text represented only as the counts of words that it contains. The (frequency of) occurrence of each word is used as a feature for training a classifier.
	Conjunctions of words (sequential, skip n-grams, other non-linear combinations) – this is typically probabilistic. 
	Higher-order linguistic structure (e.g., syntax).

Why might it be useful to predict upcoming words or assign probabilities to sentences? 
It’s very difficult to know the true probability of an arbitrary sequence of words, but we can define a language model that will give us good approximations. 

In some cases, looking at more than one word at a time might be more informative (bad versus not bad). 
An n-gram is a word sequence of length n. The main intuition is to estimate the probability of sentence P(S) by combining the probabilities of smaller parts of the sentence, which will occur more frequently. 
	1-gram or unigram: action
	2-gram or bigram: action packed
	3-gram or trigram: action packed adventure
	4-gram: action packed adventure film

We want to estimate P(S=w_1…w_n). Example: P(S=the movie was splendid).
This is really a joint probability over the words in S:
	P(w_1=the,w_2=movie,w_3=was,w_4=splendid)
For joint probability, P(X,Y)=P(Y│X)P(X)
So P(the,movie,was,splendid)
=P(splendid│the,movie,was)P(the,movie,was)
=P(splendid│the,movie,was)P(was|the,movie)P(the,movie)
=P(splendid│the,movie,was)P(was|the,movie)P(movie|the)P(the)
So, more generally 
P(w_1,…,w_n )=∏_(i=1)^n▒〖P(w_i│w_1,w_2,…w_(i-1) ) 〗
But, many of these conditional probs are just as sparse. So we make an independence assumption: the probability of a word only depends on a fixed number of previous words (history):
	Trigram model: P(w_i│w_1,w_2,…w_(i-1) )≈P(w_i│w_(i-2),w_(i-1) )
	Bigram model: P(w_i│w_1,w_2,…w_(i-1) )≈P(w_i│w_(i-1) )
	Unigram model: P(w_i│w_1,w_2,…w_(i-1) )≈P(w_i)

Why do we need text corpora?
	To evaluate our systems on:
	Good science requires controlled experimentation
	Good engineering requires benchmarks
	To help our systems work well (data-drive methods/machine learning)
	When a system’s behavior is determined solely by manual rules or databases, it is said to be rule-based, symbolic, or knowledge-driven
	Learning: collecting statistics or patterns automatically from corpora to govern the system’s behavior (dominant in most areas of contemporary NLP)
	Supervised learning: the data provides example input/output pairs
	Core behavior: training, refining behavior: tuning


SAMPLE QUESTIONS: Based on the slides that I provide in above, generate 10 multiple choice exam questions on university level in the following format:

Question 1:
EA parameters are rigid (values constant during a run).
A. True
B. False

Answer: B

Question 2:
What is the main advantage of adaptive and self-adaptive parameter control in Evolutionary Algorithms?
A. Increased user control
B. Reduction in required computing resources
C. Liberation from parameter tuning and delegating parameter setting to the evolutionary process
D. Improved predictability of parameter values

Answer: C



Generate 10 exam questions based on the provided CONTENT OF THE LECTURE SLIDES above in the format of the SAMPLE QUESTIONS:
10 QUESTIONS: 
